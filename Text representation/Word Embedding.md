N-dimensional representation of words as vectors where semantically similar words are closer together.

The problem of word embedding is that they don't take into consideration the order of words and that they cannot represent words in a large texts because such words can have multiple meaning depending of the context.

Examples: [[word2vec]], [[GloVe]], [[FastText]]